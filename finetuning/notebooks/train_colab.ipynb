{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAMP Fine-Tuning with Unsloth\n",
    "\n",
    "Fine-tune small LLMs (Llama 3.2 3B, Gemma 3 4B, Phi-4 Mini) on the LAMP dataset\n",
    "to generate valid JSON light programs from natural language.\n",
    "\n",
    "**Requirements:** Google Colab with GPU runtime (T4 or A100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install dependencies\n",
    "!pip install unsloth\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clone the repo and load data\n",
    "!git clone https://github.com/ArianMoeini/lampAI.git\n",
    "%cd lampAI/finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Choose your model\n",
    "# Options: \"llama\", \"gemma\", \"phi\"\n",
    "MODEL_CHOICE = \"llama\"  # <-- CHANGE THIS\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": {\n",
    "        \"model_name\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "        \"output_name\": \"lamp-llama-3b\",\n",
    "        \"max_seq_length\": 4096,\n",
    "    },\n",
    "    \"gemma\": {\n",
    "        \"model_name\": \"unsloth/gemma-3-4b-it\",\n",
    "        \"output_name\": \"lamp-gemma-4b\",\n",
    "        \"max_seq_length\": 4096,\n",
    "    },\n",
    "    \"phi\": {\n",
    "        \"model_name\": \"unsloth/Phi-4-mini-instruct\",\n",
    "        \"output_name\": \"lamp-phi-mini\",\n",
    "        \"max_seq_length\": 4096,\n",
    "    },\n",
    "}\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_CHOICE]\n",
    "print(f\"Selected: {config['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load model with 4-bit quantization\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=config[\"model_name\"],\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {config['model_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Configure LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({trainable/total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load and format dataset\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def read_jsonl(path):\n",
    "    items = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            items.append(json.loads(line.strip()))\n",
    "    return items\n",
    "\n",
    "train_data = Dataset.from_list(read_jsonl(\"data/train.jsonl\"))\n",
    "val_data = Dataset.from_list(read_jsonl(\"data/val.jsonl\"))\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "\n",
    "# Apply chat template\n",
    "def format_convos(examples):\n",
    "    texts = []\n",
    "    for convos in examples[\"conversations\"]:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            convos, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_data = train_data.map(format_convos, batched=True)\n",
    "val_data = val_data.map(format_convos, batched=True)\n",
    "\n",
    "# Preview one example\n",
    "print(\"\\n--- Example ---\")\n",
    "print(train_data[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train!\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_ratio=0.05,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    "    output_dir=f\"outputs/{config['output_name']}\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Quick test before exporting\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompts = [\n",
    "    \"warm and cozy\",\n",
    "    \"show a heart\",\n",
    "    \"countdown from 5\",\n",
    "    \"party mode\",\n",
    "    \"simulate a thunderstorm\",\n",
    "]\n",
    "\n",
    "system_prompt = train_data[0][\"conversations\"][0][\"content\"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Create a light program for this request.\\n\\nRequest: {prompt}\\n\\nRespond with ONLY a JSON program. No text.\"},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs, max_new_tokens=2048,\n",
    "        temperature=0.3, do_sample=True,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Check if valid JSON\n",
    "    try:\n",
    "        parsed = json.loads(response)\n",
    "        status = \"VALID JSON\"\n",
    "    except:\n",
    "        status = \"INVALID\"\n",
    "    \n",
    "    print(f\"\\n[{status}] {prompt}\")\n",
    "    print(f\"  {response[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Export to GGUF for Ollama\n",
    "import os\n",
    "os.makedirs(\"exports\", exist_ok=True)\n",
    "\n",
    "# Export Q4_K_M (recommended for Pi 5)\n",
    "model.save_pretrained_gguf(\n",
    "    f\"exports/{config['output_name']}\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "print(f\"GGUF Q4_K_M saved to exports/{config['output_name']}\")\n",
    "\n",
    "# Also export Q8_0 as backup\n",
    "model.save_pretrained_gguf(\n",
    "    f\"exports/{config['output_name']}-q8\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q8_0\",\n",
    ")\n",
    "print(f\"GGUF Q8_0 saved to exports/{config['output_name']}-q8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Create Ollama Modelfile\n",
    "modelfile_content = f\"\"\"FROM ./{config['output_name']}-unsloth.Q4_K_M.gguf\n",
    "PARAMETER temperature 0.3\n",
    "PARAMETER num_predict 4096\n",
    "PARAMETER stop <|eot_id|>\n",
    "PARAMETER stop <end_of_turn>\n",
    "\"\"\"\n",
    "\n",
    "with open(f\"exports/Modelfile.{config['output_name']}\", \"w\") as f:\n",
    "    f.write(modelfile_content)\n",
    "\n",
    "print(f\"Modelfile saved!\")\n",
    "print(f\"\\nTo deploy on your Pi 5:\")\n",
    "print(f\"  1. Copy the GGUF file and Modelfile to your Pi\")\n",
    "print(f\"  2. Run: ollama create {config['output_name']} -f Modelfile.{config['output_name']}\")\n",
    "print(f\"  3. Test: ollama run {config['output_name']} 'warm and cozy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Download the GGUF files\n",
    "# On Colab, this will trigger a browser download\n",
    "from google.colab import files\n",
    "\n",
    "import glob\n",
    "gguf_files = glob.glob(\"exports/**/*.gguf\", recursive=True)\n",
    "for f in gguf_files:\n",
    "    print(f\"Downloading: {f} ({os.path.getsize(f) / 1e9:.1f} GB)\")\n",
    "    files.download(f)\n",
    "\n",
    "# Also download the Modelfile\n",
    "modelfiles = glob.glob(\"exports/Modelfile.*\")\n",
    "for f in modelfiles:\n",
    "    files.download(f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
